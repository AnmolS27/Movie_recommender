{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXj9mx2rtGRC"
      },
      "outputs": [],
      "source": [
        "! pip install surprise\n",
        "\n",
        "# Importing liberaries\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from ast import literal_eval  # evaluate strings containing Python code in the current Python environment\n",
        "from nltk.stem.snowball import SnowballStemmer # Removing stem words\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # To convert text to numerical data\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "from surprise import Reader, Dataset, SVD\n",
        "from surprise.model_selection import cross_validate\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "\n",
        "import warnings  # disable python warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#link\n",
        "! gdown --id 1_BM6tAHriNzdVJTaKn_xNjTNDDvXse1L\n",
        "#credits\n",
        "! gdown --id 1jBSHwmSiprGTDzyCBxDTPp_pnGWcKtWq\n",
        "#rating_small\n",
        "! gdown --id 1jlxlS8C-BkN2-7fGbZhGNNHLqrXkKBoo\n",
        "#movie_metadata\n",
        "! gdown --id 1wb-IR-M-1CsJZXv7n-oCYsHcqflA7yGU\n",
        "#keywords\n",
        "! gdown --id 1zYCVCIsDofDXJ2xnrN5DNDzqG21kCjKN"
      ],
      "metadata": {
        "id": "E3njr3XVtT9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading datasets \n",
        "\n",
        "movies_data = pd.read_csv(\"movies_metadata.csv\", low_memory=False)\n",
        "credits = pd.read_csv('credits.csv')\n",
        "keywords = pd.read_csv('keywords.csv')\n",
        "links_small = pd.read_csv('links_small.csv')\n",
        "ratings = pd.read_csv(\"ratings_small.csv\")"
      ],
      "metadata": {
        "id": "jiC_daENtr80"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for null values in the dataset\n",
        "\n",
        "print(movies_data.isnull().sum(),'\\n') # We have used only selected column of the dataset which include genres,id,vote_average,vote_count\n",
        "print(links_small.isnull().sum(),'\\n')\n",
        "print(ratings.isnull().sum(),'\\n')\n",
        "print(keywords.isnull().sum(),'\\n')\n",
        "print(credits.isnull().sum(),'\\n')"
      ],
      "metadata": {
        "id": "VOiVua9Jtt-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the rows with null value in the vote_average and vote_count columns in movies_data dataframe\n",
        "\n",
        "movies_data = movies_data.dropna(subset=['vote_average', 'vote_count'])\n",
        "print(movies_data.isnull().sum(),'\\n')"
      ],
      "metadata": {
        "id": "Sf9ebFrdtzEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Recommender System"
      ],
      "metadata": {
        "id": "4SiK13-pt80Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Recommender (Top movies irrespective of genres)\n",
        "\n",
        "# Weighted rating\n",
        "def weighted_rating(v,R):\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    This function calculate weighted rating of a movies using IMDB formula\n",
        "    \n",
        "    Parameters: v (int): vote count\n",
        "                R (int): vote average\n",
        "    Returns: (float) IMDB score\n",
        "    \n",
        "    '''\n",
        "    return ((v/(v+m)) * R) + ((m/(m+v)) * C)  \n",
        "\n",
        "\n",
        "\n",
        "C = movies_data['vote_average'].mean()         # mean vote across all data\n",
        "m = movies_data['vote_count'].quantile(0.95)   # movies with more than 95% votes is taken (95 percentile)\n",
        "\n",
        "# Taking movies whose vote count is greater than m\n",
        "top_movies = movies_data.copy().loc[movies_data['vote_count'] >= m]\n",
        "top_movies = top_movies.reset_index()\n",
        "\n",
        "top_movies['score'] = ''\n",
        "\n",
        "for i in range(top_movies.shape[0]):\n",
        "    v = top_movies['vote_count'][i]          # number of vote count of the movie\n",
        "    R = top_movies['vote_average'][i]        # average rating of the movie\n",
        "    top_movies['score'][i] = weighted_rating(v,R)\n",
        "\n",
        "top_movies = top_movies.sort_values('score', ascending=False)  # sorting movies in descending order according to score\n",
        "top_movies = top_movies.reset_index()\n",
        "\n",
        "# top_movies[['title', 'vote_count', 'vote_average', 'score']].head(20) # top 20 movies\n",
        "t1 = top_movies[['title', 'score']].head(20)\n",
        "\n",
        "print(t1)"
      ],
      "metadata": {
        "id": "EBOzP6tjuA92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of average vote among movies in the dataset\n",
        "\n",
        "fig = px.histogram(top_movies, x=\"vote_average\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "7sTN3PQiuHc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple recommender based on genres\n",
        "\n",
        "genres = set()\n",
        "\n",
        "# Finding the exhaustive set of genres in the dataset \n",
        "top_movies['genres'] = top_movies['genres'].apply(literal_eval)\n",
        "for i in range(top_movies['genres'].shape[0]):   # converting string in map\n",
        "    for x in top_movies['genres'][i]:\n",
        "        genres.add(x['name'])\n",
        "        \n",
        "        \n",
        "# creating map of string (genre name) and movies names(dataframe)\n",
        "genres_based = dict()   \n",
        "for i in range(top_movies['genres'].shape[0]):  \n",
        "    for x in top_movies['genres'][i]:\n",
        "        if x['name'] not in genres_based.keys():\n",
        "            genres_based[x['name']] = pd.DataFrame(columns = top_movies.columns)\n",
        "        genres_based[x['name']] = genres_based[x['name']].append(top_movies.iloc[i])  "
      ],
      "metadata": {
        "id": "iLGPzOMPuLpY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing frequency of occurence of different genres\n",
        "\n",
        "# Creating a count vector (list) containing frequency of a perticular genre\n",
        "cnt = list()\n",
        "for i in genres:\n",
        "    cnt.append(genres_based[i].shape[0])\n",
        "    \n",
        "# Making a datafram \n",
        "genre_cnt = pd.DataFrame( { 'genres' : list(genres),\n",
        "                            'count'  : cnt\n",
        "    \n",
        "},\n",
        "                         columns = ['genres','count']\n",
        ")\n",
        "\n",
        "fig = px.bar(genre_cnt, x='genres', y='count')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "tqXS9_5nuNyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genres_based_rcmnd(name):\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    This function returns the top 10 movies of the given genre\n",
        "    \n",
        "    Parameters: name (string): Name of the genre\n",
        "    \n",
        "    Returns: (Dataframe) Top 10 move recommendation\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    if name not in genres:\n",
        "        return None\n",
        "    else:\n",
        "        return genres_based[name][['title', 'vote_count', 'vote_average', 'score']].head(10)\n",
        "\n",
        "\n",
        "print(genres_based_rcmnd(\"Comedy\"))"
      ],
      "metadata": {
        "id": "SsvPFz9_uRqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing the data\n",
        "\n",
        "movies_data['id'] = movies_data['id'].astype('int')  #The astype() function is used to cast a pandas object to a specified data type.\n",
        "\n",
        "# merging both credits and keywords in movies_data on the basis of movie id\n",
        "movies_data = movies_data.merge(credits, on='id')\n",
        "movies_data = movies_data.merge(keywords, on='id')\n",
        "\n",
        "links_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')\n",
        "\n",
        "# taking only those movies whos id is present in link_small because of limited computing power\n",
        "smd = movies_data[movies_data['id'].isin(links_small)]  \n",
        "smd = smd.reset_index()\n",
        "\n",
        "smd.head()"
      ],
      "metadata": {
        "id": "v5r-t37huSZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_director(x):\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    This function gives the name of first director occuring in the crew of the movie\n",
        "    \n",
        "    Parameters: x(list of dictionary): List containing name and corrosponding role of complete cast of the movie\n",
        "    \n",
        "    Returns: (string) It returns the first director name that appear in the list\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    for i in x:\n",
        "        if i['job'] == 'Director':\n",
        "            return i['name']\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "# Applying literal_eval to get the right data type from the expression of string\n",
        "smd['cast'] = smd['cast'].apply(literal_eval)\n",
        "smd['crew'] = smd['crew'].apply(literal_eval)\n",
        "smd['keywords'] = smd['keywords'].apply(literal_eval)\n",
        "smd['genres'] = smd['genres'].apply(literal_eval)\n",
        "\n",
        "smd['director'] = smd['crew'].apply(get_director) \n",
        "\n",
        "# Taking all the movie cast in a list and then taking only the top 3 cast\n",
        "smd['cast'] = smd['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n",
        "smd['cast'] = smd['cast'].apply(lambda x: x[:3] if len(x) >=3 else x)\n",
        "smd['cast'] = smd['cast'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])# Strip Spaces and Convert to Lowercase\n",
        "\n",
        "smd['keywords'] = smd['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n",
        "\n",
        "smd['genres'] = smd['genres'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n",
        "smd['genres'] = smd['genres'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])\n",
        "\n",
        "smd['director'] = smd['director'].astype('str').apply(lambda x: str.lower(x.replace(\" \", \"\")))\n",
        "smd['director'] = smd['director'].apply(lambda x: [x,x,x])  # giving more weight to the director relative to the entire cast"
      ],
      "metadata": {
        "id": "CTyOOm_FuVUa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_keywords(x):\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    This funtion remove those keywords which occur only once \n",
        "    \n",
        "    Parameters: x(list): List containing keywords of the movie\n",
        "    \n",
        "    Returns: (list) It returns a list containg only those keywords which are present in keywords_count ( it is a dictionary containg those keywords which occur more than once )\n",
        "    \n",
        "    '''    \n",
        "    \n",
        "    words = []\n",
        "    for i in x:\n",
        "        if i in keywords_count.keys():\n",
        "            words.append(i)\n",
        "    return words\n",
        "\n",
        "\n",
        "# Creating the count of every keywords\n",
        "keywords_count = dict()\n",
        "for i in range(len(smd['keywords'])):\n",
        "    for j in range(len(smd['keywords'][i])):\n",
        "        if smd['keywords'][i][j] not in keywords_count.keys():\n",
        "            keywords_count[smd['keywords'][i][j]] = 0\n",
        "        keywords_count[smd['keywords'][i][j]] +=1\n",
        "\n",
        "# removing those keywords which occur only once\n",
        "for i in list(keywords_count):\n",
        "    if keywords_count[i] == 1:\n",
        "        del keywords_count[i]"
      ],
      "metadata": {
        "id": "AM5xWde1ucGe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "\n",
        "# Stemming the words \n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "smd['keywords'] = smd['keywords'].apply(filter_keywords) # removing those keywords which occur only once\n",
        "smd['keywords'] = smd['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "smd['keywords'] = smd['keywords'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])\n",
        "\n",
        "# combining keywords, cast, director and genres\n",
        "smd['soup'] = smd['keywords'] + smd['cast'] + smd['director'] + smd['genres']\n",
        "smd['soup'] = smd['soup'].apply(lambda x: ' '.join(x))\n",
        "smd['soup'][0] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MNkbFm2EucsK",
        "outputId": "a05742e9-aa32-4956-b69c-06da47d03235"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'jealousi toy boy friendship friend rivalri boynextdoor newtoy toycomestolif tomhanks timallen donrickles johnlasseter johnlasseter johnlasseter animation comedy family'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Wordcloud for visualisation of the word which occur frequently in the dataset\n",
        "\n",
        "# Combining all the text contained in smd['soup'] column\n",
        "text = \"\"\n",
        "for i in smd['soup']:\n",
        "    text +=i\n",
        "    \n",
        "word_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "# plot the WordCloud image                      \n",
        "plt.figure(figsize = (8, 8))\n",
        "plt.imshow(word_cloud)\n",
        "plt.axis(\"off\")\n",
        " \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wm6mQKZBugON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = CountVectorizer(analyzer='word',ngram_range=(1, 2) ,min_df=0, stop_words='english')\n",
        "count_matrix = count.fit_transform(smd['soup'])\n",
        "\n",
        "cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
        "\n",
        "titles = smd['title']\n",
        "indices = pd.Series(smd.index, index=smd['title'])  # Creating a mapping between movie and title and index"
      ],
      "metadata": {
        "id": "wbufyoJguldu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cosine=pd.DataFrame(cosine_sim)\n",
        "df_cosine"
      ],
      "metadata": {
        "id": "nFaNDzu-un8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Heatmap for visualization of correlation between different movies \n",
        "\n",
        "#specify size of heatmap\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "#create seaborn heatmap of only top 100 movies\n",
        "sns.heatmap(cosine_sim[:100,:100])"
      ],
      "metadata": {
        "id": "k2niHMrOuqf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph structure to visualize similarity relation between selected movies\n",
        "\n",
        "g = nx.Graph()\n",
        "n = 10\n",
        "\n",
        "for i in range(n):\n",
        "    g.add_node(titles[i])\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        if i != j and cosine_sim[i][j]>0:\n",
        "            g.add_edge(titles[i],titles[j],weight = cosine_sim[i][j])\n",
        "    \n",
        "g = g.to_undirected()\n",
        "pos = nx.spring_layout(g)\n",
        "nx.draw_networkx_nodes(g, pos, node_size = 20)\n",
        "nx.draw_networkx_edges(g, pos,alpha = 0.3)\n",
        "nx.draw_networkx_labels(g, pos, font_size=10, horizontalalignment=\"right\")\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jVy2Oos0us1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices"
      ],
      "metadata": {
        "id": "ZlePmn72uzcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommendations(title):\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    This function gives the top 10 movies according to the cosine similarities calculated above along with the movie id\n",
        "    \n",
        "    Parameters: title (string) : Name of the movie present in the smd dataset\n",
        "    \n",
        "    Returns: (list) Top 10 movies along with the movie id\n",
        "    \n",
        "    \n",
        "    '''\n",
        "    \n",
        "    idx = indices[title] # movie id corrosponding to the given title \n",
        "    sim_scores = list(enumerate(cosine_sim[idx])) # list of cosine similarity scores value along the given index\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) # sorting the given scores in ascending order\n",
        "    sim_scores = sim_scores[1:31] # Taking only the top 30 scores\n",
        "    movie_indices = [i[0] for i in sim_scores] # Finding the indices of 30 most similar movies\n",
        "    return titles.iloc[movie_indices] \n",
        "\n",
        "get_recommendations('The Dark Knight').head(10)"
      ],
      "metadata": {
        "id": "Kx8OZ06wuz-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collaborative Filtering"
      ],
      "metadata": {
        "id": "LgSNmMfKu5v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the timestamp column since we dont need it now\n",
        "ratings = ratings.drop(columns=\"timestamp\")\n",
        "\n",
        "#checking for missing values\n",
        "ratings.isna().sum()\n",
        "\n",
        "#check for the numbers of total movies and users\n",
        "movies= ratings['movieId'].nunique()  #nunique is similar to count but only takes unique values\n",
        "users=ratings['userId'].nunique()\n",
        "print('total number of movies =', movies)\n",
        "print('total number of users =', users)\n",
        "\n",
        "#  HIstogram showing frequency of ratings given by different users\n",
        "fig = px.histogram(ratings, x=\"rating\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "6lDaj2unu9DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# columns to use for training\n",
        "columns = ['userId','movieId','rating']\n",
        "\n",
        "# create reader from surprise \n",
        "# the rating should lie in the provided scale\n",
        "reader = Reader(rating_scale =(0.5,5))\n",
        "\n",
        "#create dataset from dataframe\n",
        "data = Dataset.load_from_df(ratings[columns],reader)\n",
        "\n",
        "# create trainset ie the data which is present (ratings of those movies which are rated by respective users)\n",
        "trainset = data.build_full_trainset()\n",
        "\n",
        "# create testset, here the anti_testset is testset\n",
        "# data containing users movie pairs which are not rated by that particular user\n",
        "testset = trainset.build_anti_testset()\n",
        " \n",
        "\n",
        "model = SVD(n_epochs = 25, verbose = True) #n_epochs:The number of iteration of the SGD(simple gradient descent) procedure. Default is 20\n",
        "                                           #verbose:If True, prints the current epoch. Default is False.\n",
        "    \n",
        "cross_validate(model, data, measures=['RMSE','MAE'], cv= 5, verbose= True)\n",
        "print('Training Done')\n",
        "\n",
        "#prediction\n",
        "prediction = model.test(testset)"
      ],
      "metadata": {
        "id": "7zWmMWHgu_Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = { 'userId' : [99999,99999,99999,99999,99999],\n",
        "           'movieId' : [31,1029,1293,1172,1343],\n",
        "           'rating'  : [3.0, 4.5, 1.2, 3.3,2]\n",
        "    \n",
        "}\n",
        "\n",
        "df = pd.DataFrame(example)\n",
        "frames = [ratings, df]\n",
        "result = pd.concat(frames)"
      ],
      "metadata": {
        "id": "d-UYSVIfvC7N"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create dataset from dataframe\n",
        "data= Dataset.load_from_df(result[columns],reader)\n",
        "\n",
        "#create trainset\n",
        "trainset= data.build_full_trainset()\n",
        "\n",
        "#create testset, here the anti_testset is testset\n",
        "testset = trainset.build_anti_testset()\n",
        "\n",
        "cross_validate(model,data, measures=['RMSE','MAE'], cv= 5, verbose= True) #cv is the number of parts in which data will be divided.\n",
        "print('Training Done')\n",
        "\n",
        "#prediction\n",
        "prediction = model.test(testset)\n",
        "prediction[99999]"
      ],
      "metadata": {
        "id": "2jZiWw6KvJrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n(prediction, n):\n",
        "    \n",
        "    '''\n",
        "    This function recommend users with top n movies based on prediction calculated using the surprise library\n",
        "    \n",
        "    Parameters: prediction(list): This contains (user, movie) rating prediction for all user movie pairs\n",
        "                n(int): Number of recommendations\n",
        "    \n",
        "    Results: Returns top 30 movies along with movie id for all users\n",
        "    \n",
        "    \n",
        "    '''\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    top_n = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in prediction: \n",
        "        top_n[uid].append((iid, est))\n",
        "\n",
        "    # Then sort the predictions for each user and retrieve the n highest ones.\n",
        "    for uid, user_ratings in top_n.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n[uid] = user_ratings[:n]\n",
        "\n",
        "    return top_n\n",
        "\n",
        "rcmnd = []\n",
        "top_n = get_top_n(prediction, n=30)\n",
        "for uid, user_ratings in top_n.items():\n",
        "    if uid == 99999:\n",
        "        for (iid,rating) in user_ratings:\n",
        "            for i in range(movies_data.shape[0]):\n",
        "                if movies_data['id'][i] == iid:\n",
        "                    rcmnd.append([movies_data['id'][i],movies_data['title'][i]])\n",
        "        break"
      ],
      "metadata": {
        "id": "EDPawKf5vNae"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rcmnd[:10]"
      ],
      "metadata": {
        "id": "VWCzQ1e1vOCF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}